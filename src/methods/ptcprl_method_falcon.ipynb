{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import Trainer\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch, time, itertools, json\n",
    "from datasets import load_dataset\n",
    "from transformers import (AutoTokenizer,\n",
    "                          AutoModelForSequenceClassification,\n",
    "                          TrainingArguments, Trainer)\n",
    "from peft import PrefixTuningConfig, TaskType, get_peft_model\n",
    "from evaluate import load as load_metric\n",
    "from torch.distributions import Normal\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "metric = load_metric(\"accuracy\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainerCallback, get_scheduler\n",
    "\n",
    "class Phase2LRScheduler(TrainerCallback):\n",
    "    def __init__(self, phase1_epochs: int, phase2_lr: float, warmup_ratio: float):\n",
    "        super().__init__()\n",
    "        self.phase1_epochs = phase1_epochs\n",
    "        self.phase2_lr     = phase2_lr\n",
    "        self.warmup_ratio  = warmup_ratio\n",
    "        self.trainer       = None\n",
    "\n",
    "    def on_epoch_begin(self, args, state, control, **kwargs):\n",
    "        if int(state.epoch) == self.phase1_epochs and self.trainer:\n",
    "            for g in self.trainer.optimizer.param_groups:\n",
    "                g[\"lr\"] = self.phase2_lr\n",
    "\n",
    "            steps_per_epoch = len(self.trainer.get_train_dataloader())\n",
    "            remaining_epochs = args.num_train_epochs - self.phase1_epochs\n",
    "            total_steps_phase2 = remaining_epochs * steps_per_epoch\n",
    "            num_warmup = int(self.warmup_ratio * total_steps_phase2)\n",
    "\n",
    "            self.trainer.lr_scheduler = get_scheduler(\n",
    "                name=\"cosine\",\n",
    "                optimizer=self.trainer.optimizer,\n",
    "                num_warmup_steps=num_warmup,\n",
    "                num_training_steps=total_steps_phase2\n",
    "            )\n",
    "\n",
    "            print(f\"\\n>>> Phase 2 START: reset LR to {self.phase2_lr} and restart cosine scheduler \"\n",
    "                  f\"({total_steps_phase2} steps, {num_warmup} warmup)\\n\")\n",
    "        return control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "from datasets import load_dataset\n",
    "from transformers import DataCollatorWithPadding\n",
    "from transformers import AutoConfig\n",
    "from peft import PrefixTuningConfig, get_peft_model\n",
    "def prepare_for_training(\n",
    "        dataset_name, \n",
    "        model_name,\n",
    "        text_column_name,\n",
    "        label_column_name,\n",
    "        num_virtual_tokens=20, \n",
    "        max_length=128):\n",
    "    if dataset_name == \"sst2\":\n",
    "        raw = load_dataset(\"glue\", dataset_name)\n",
    "    else:\n",
    "        raw = load_dataset(dataset_name, trust_remote_code=True)\n",
    "\n",
    "    tok = AutoTokenizer.from_pretrained(model_name)\n",
    "    if model_name != \"roberta-base\":\n",
    "        tok.pad_token = tok.eos_token\n",
    "    def prep(x):\n",
    "        t = tok(x[text_column_name], padding=\"max_length\", truncation=True, max_length=max_length)\n",
    "        t[\"labels\"]=x[label_column_name]; \n",
    "        return t\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tok, return_tensors=\"pt\")\n",
    "    neseccary_cols = [\"input_ids\",\"attention_mask\",\"labels\"]\n",
    "    ds = raw[\"train\"].shuffle(seed=42).map(\n",
    "        prep, \n",
    "        batched=True, \n",
    "        remove_columns=set(raw[\"train\"].features.keys()).difference(set(neseccary_cols))\n",
    "    )\n",
    "    ds.set_format(\"torch\",columns=neseccary_cols)\n",
    "\n",
    "    split = ds.train_test_split(0.1, shuffle=True)\n",
    "    train_main = split[\"train\"]\n",
    "    val_ds  = split[\"test\"]\n",
    "    split2 = val_ds.train_test_split(test_size=100, shuffle=True)\n",
    "    val_ds = split2[\"train\"]\n",
    "    rl_subset = split2[\"test\"]\n",
    "    test_ds = raw.get('validation') or raw.get(\"test\")\n",
    "    test_ds = test_ds.map(\n",
    "        prep, \n",
    "        batched=True, \n",
    "        remove_columns=set(raw[\"train\"].features.keys()).difference(set(neseccary_cols))\n",
    "    )\n",
    "    test_ds.set_format(\"torch\",columns=neseccary_cols)\n",
    "\n",
    "    num_labels = len(raw[\"train\"].features[label_column_name].names)\n",
    "    \n",
    "    if model_name != \"roberta-base\":\n",
    "        config = AutoConfig.from_pretrained(\n",
    "            \"tiiuae/falcon-rw-1b\",\n",
    "            hidden_size=768, \n",
    "            num_hidden_layers=6, \n",
    "            num_attention_heads=12,\n",
    "            num_key_value_heads=2\n",
    "        )\n",
    "\n",
    "        base = AutoModelForSequenceClassification.from_config(\n",
    "            config,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "        )\n",
    "        tok.pad_token = tok.eos_token\n",
    "        base.config.pad_token_id = tok.pad_token_id\n",
    "    else:\n",
    "        base = AutoModelForSequenceClassification.from_pretrained(\n",
    "            model_name,\n",
    "            num_labels=num_labels,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "        )\n",
    "    if model_name == \"roberta-base\":\n",
    "        for p in base.roberta.parameters(): \n",
    "            p.requires_grad=False\n",
    "    else:\n",
    "        for p in base.parameters(): \n",
    "            p.requires_grad=False\n",
    "\n",
    "\n",
    "    cfg = PrefixTuningConfig(\n",
    "        task_type=\"SEQ_CLS\",\n",
    "        num_virtual_tokens=num_virtual_tokens,\n",
    "        prefix_projection=False\n",
    "    )\n",
    "    model = get_peft_model(base,cfg)\n",
    "    return train_main, val_ds, rl_subset, test_ds, model, data_collator, tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TwoPhaseTrainer(Trainer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *args,\n",
    "        phase1_epochs: int = 3,\n",
    "        alpha: float = 1.0,\n",
    "        beta: float  = 0.3, \n",
    "        gamma: float = 0.05,  \n",
    "        rl_subset_size: int = 32,\n",
    "        rl_dataset=None,\n",
    "        k_negatives: int = 1,\n",
    "        sigma: float = 0.00002,\n",
    "        **kwargs\n",
    "    ):\n",
    "        self.phase1_epochs  = phase1_epochs\n",
    "        self.alpha          = alpha\n",
    "        self.beta           = beta\n",
    "        self.gamma          = gamma\n",
    "        self.rl_subset_size = rl_subset_size\n",
    "        self.rl_dataset     = rl_dataset\n",
    "        self.k_negatives    = k_negatives\n",
    "        self.sigma          = sigma\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        subset = list(range(min(len(self.rl_dataset), self.rl_subset_size)))\n",
    "        self._rl_loader = DataLoader(\n",
    "            self.rl_dataset.select(subset),\n",
    "            batch_size=self.args.per_device_train_batch_size,\n",
    "            collate_fn=self.data_collator\n",
    "        )\n",
    "        self.prefix_params = []\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if \"prompt_encoder\" in name:\n",
    "                self.prefix_params.append((name, param))\n",
    "\n",
    "    def mutate_prefix_for_contrastive(self):\n",
    "        negs = []\n",
    "        for _ in range(self.k_negatives):\n",
    "            mutated = {}\n",
    "            for name, param in self.prefix_params:\n",
    "                mask = (torch.rand_like(param) > 0.1).float()\n",
    "                mutated[name] = param * mask\n",
    "            negs.append(mutated)\n",
    "        return negs\n",
    "\n",
    "    def compute_contrastive_loss(self, logits_pos, logits_negs, temp=1.0):\n",
    "        pos_norm = F.normalize(logits_pos, dim=-1)\n",
    "        neg_norms = [F.normalize(n, dim=-1) for n in logits_negs]\n",
    "        sim_pos = (pos_norm * pos_norm).sum(-1) / temp\n",
    "        sim_negs = torch.stack([(pos_norm*neg).sum(-1) for neg in neg_norms], dim=1) / temp\n",
    "        loss = -torch.log(sim_pos.exp() / (sim_pos.exp() + sim_negs.exp().sum(1))).mean()\n",
    "        return loss\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def compute_reward(self, model):\n",
    "        was_train = model.training\n",
    "        model.eval()\n",
    "        correct = total = 0\n",
    "        for batch in self._rl_loader:\n",
    "            batch = {k: v.to(model.device) for k,v in batch.items()}\n",
    "            out = model(**batch).logits.argmax(-1)\n",
    "            correct += (out == batch[\"labels\"]).sum().item()\n",
    "            total += out.size(0)\n",
    "        if was_train: model.train()\n",
    "        return correct/total\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        outputs   = model(**inputs)\n",
    "        loss_mle  = outputs.loss\n",
    "        if (self.state.epoch < self.phase1_epochs) or not model.training:\n",
    "            return (loss_mle, outputs) if return_outputs else loss_mle\n",
    "\n",
    "        logits_pos = outputs.logits\n",
    "        original = {n: p.data.clone() for n,p in self.prefix_params}\n",
    "\n",
    "        neg_prefixes = self.mutate_prefix_for_contrastive()\n",
    "        logits_negs = []\n",
    "        for neg in neg_prefixes:\n",
    "            with torch.no_grad():\n",
    "                for name, param in self.prefix_params:\n",
    "                    param.data.copy_(neg[name].data)\n",
    "            logits_negs.append(model(**inputs).logits.detach())\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for name, param in self.prefix_params:\n",
    "                param.data.copy_(original[name])\n",
    "\n",
    "        loss_contrast = self.compute_contrastive_loss(logits_pos, logits_negs)\n",
    "\n",
    "\n",
    "        if self.state.global_step % 100 == 0:\n",
    "            log_probs = []\n",
    "\n",
    "            for name, param in self.prefix_params:\n",
    "                eps   = torch.randn_like(param) * self.sigma\n",
    "                noisy = param + eps\n",
    "                dist  = Normal(loc=param, scale=self.sigma)\n",
    "                log_probs.append(dist.log_prob(noisy).sum())\n",
    "                with torch.no_grad():\n",
    "                    param.data.copy_(noisy.data)\n",
    "\n",
    "            reward = self.compute_reward(model)\n",
    "\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for name, param in self.prefix_params:\n",
    "                    param.data.copy_(original[name])\n",
    "\n",
    "            total_log_prob = torch.stack(log_probs).mean()\n",
    "            loss_rl = - reward * total_log_prob\n",
    "        else:\n",
    "            loss_rl = 0.0\n",
    "\n",
    "        loss = self.alpha*loss_mle + self.beta*loss_contrast + self.gamma*loss_rl\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_trainer(model, \n",
    "                    train_ds, \n",
    "                    val_ds, \n",
    "                    rl_subset, \n",
    "                    data_collator, \n",
    "                    tok, \n",
    "                    alpha=1.0, \n",
    "                    beta=0.1, \n",
    "                    gamma=0.0002,\n",
    "                    rl_subset_size=32,\n",
    "                    k_negatives=7,\n",
    "                    phase1_epochs=3,\n",
    "                    learning_rate=1e-3,\n",
    "                    warmup_ratio=0.1,\n",
    "                    per_device_train_batch_size=16,\n",
    "                    per_device_eval_batch_size=32,\n",
    "                    num_train_epochs=5,\n",
    "                    lr_scheduler_type=\"cosine\",\n",
    "                    sigma=0.1,\n",
    "                    ):\n",
    "\n",
    "    def compute_metrics(eval_pred):\n",
    "        logits, labels = eval_pred\n",
    "        preds = logits.argmax(-1)\n",
    "        return metric.compute(predictions=preds, references=labels)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"two_phase\",\n",
    "        num_train_epochs=num_train_epochs,\n",
    "        per_device_train_batch_size=per_device_train_batch_size,\n",
    "        per_device_eval_batch_size=per_device_eval_batch_size,\n",
    "        learning_rate=learning_rate,\n",
    "        lr_scheduler_type=lr_scheduler_type,\n",
    "        warmup_ratio=warmup_ratio,\n",
    "        eval_strategy=\"steps\",\n",
    "        eval_steps = 1000,\n",
    "        metric_for_best_model=\"accuracy\",\n",
    "        fp16=True,\n",
    "        report_to=\"none\",\n",
    "    )\n",
    "    lr_callback = Phase2LRScheduler(\n",
    "        phase1_epochs=phase1_epochs, \n",
    "        phase2_lr=learning_rate, \n",
    "        warmup_ratio=warmup_ratio\n",
    "    )\n",
    "\n",
    "    trainer = TwoPhaseTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_ds,\n",
    "        eval_dataset=val_ds,\n",
    "        rl_dataset=rl_subset,\n",
    "        phase1_epochs=phase1_epochs,\n",
    "        callbacks=[lr_callback],\n",
    "        alpha=alpha, beta=beta, gamma=gamma,\n",
    "        rl_subset_size=rl_subset_size,\n",
    "        k_negatives=k_negatives,\n",
    "        compute_metrics=compute_metrics,\n",
    "        data_collator=data_collator,\n",
    "        sigma=sigma\n",
    "    )\n",
    "\n",
    "    lr_callback.trainer = trainer\n",
    "    return trainer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "train_ds, val_ds, rl_subset, test_ds, model, data_collator, tok \\\n",
    "    = prepare_for_training(\"trec\", \"tiiuae/falcon-rw-1b\", \"text\", \"coarse_label\", num_virtual_tokens=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "trainer_trec = prepare_trainer(model, train_ds, val_ds, rl_subset, data_collator, tok, phase1_epochs=15, num_train_epochs=20, per_device_train_batch_size=2, per_device_eval_batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='49060' max='49060' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [49060/49060 39:47, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.752400</td>\n",
       "      <td>1.655549</td>\n",
       "      <td>0.320628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.589600</td>\n",
       "      <td>1.302030</td>\n",
       "      <td>0.582960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.091500</td>\n",
       "      <td>0.788590</td>\n",
       "      <td>0.751121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.777000</td>\n",
       "      <td>0.643710</td>\n",
       "      <td>0.820628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.691500</td>\n",
       "      <td>0.545119</td>\n",
       "      <td>0.852018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.549000</td>\n",
       "      <td>0.479054</td>\n",
       "      <td>0.867713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.486900</td>\n",
       "      <td>0.445813</td>\n",
       "      <td>0.881166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.441700</td>\n",
       "      <td>0.415156</td>\n",
       "      <td>0.885650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.426500</td>\n",
       "      <td>0.408092</td>\n",
       "      <td>0.899103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.365600</td>\n",
       "      <td>0.367892</td>\n",
       "      <td>0.912556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.378900</td>\n",
       "      <td>0.346142</td>\n",
       "      <td>0.914798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.367700</td>\n",
       "      <td>0.337466</td>\n",
       "      <td>0.917040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.357900</td>\n",
       "      <td>0.340567</td>\n",
       "      <td>0.914798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.344300</td>\n",
       "      <td>0.327093</td>\n",
       "      <td>0.921525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.340900</td>\n",
       "      <td>0.321090</td>\n",
       "      <td>0.919283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>-0.432600</td>\n",
       "      <td>0.328034</td>\n",
       "      <td>0.924726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>-0.458900</td>\n",
       "      <td>0.320820</td>\n",
       "      <td>0.933767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>-0.455800</td>\n",
       "      <td>0.303429</td>\n",
       "      <td>0.939735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>-0.492200</td>\n",
       "      <td>0.286968</td>\n",
       "      <td>0.944978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>-0.500800</td>\n",
       "      <td>0.287994</td>\n",
       "      <td>0.944978</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Phase 2 START: reset LR to 0.001 and restart cosine scheduler (12265 steps, 1226 warmup)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=49060, training_loss=0.4011252948715888, metrics={'train_runtime': 2387.9889, 'train_samples_per_second': 41.089, 'train_steps_per_second': 20.544, 'total_flos': 6454345983528960.0, 'train_loss': 0.4011252948715888, 'epoch': 20.0})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_trec.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [250/250 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.2305237352848053,\n",
       " 'eval_accuracy': 0.961,\n",
       " 'eval_runtime': 2.7524,\n",
       " 'eval_samples_per_second': 181.658,\n",
       " 'eval_steps_per_second': 90.829,\n",
       " 'epoch': 20.0}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_trec.evaluate(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "train_ds, val_ds, rl_subset, test_ds, model, data_collator, tok \\\n",
    "    = prepare_for_training(\"ag_news\", \"tiiuae/falcon-rw-1b\", \"text\", \"label\", num_virtual_tokens=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "trainer_ag_news = prepare_trainer(model, train_ds, val_ds, rl_subset, data_collator, tok, k_negatives=4, num_train_epochs=2, phase1_epochs=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13500' max='13500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13500/13500 17:11, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.385100</td>\n",
       "      <td>1.374372</td>\n",
       "      <td>0.347731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.593900</td>\n",
       "      <td>0.392951</td>\n",
       "      <td>0.880924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.370300</td>\n",
       "      <td>0.336402</td>\n",
       "      <td>0.895966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.355500</td>\n",
       "      <td>0.317509</td>\n",
       "      <td>0.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.339700</td>\n",
       "      <td>0.303450</td>\n",
       "      <td>0.905462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.339000</td>\n",
       "      <td>0.296863</td>\n",
       "      <td>0.904706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.055000</td>\n",
       "      <td>0.288166</td>\n",
       "      <td>0.918908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>-0.353500</td>\n",
       "      <td>0.279136</td>\n",
       "      <td>0.920840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>-0.394000</td>\n",
       "      <td>0.283998</td>\n",
       "      <td>0.920756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>-0.382500</td>\n",
       "      <td>0.277851</td>\n",
       "      <td>0.922605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>-0.372400</td>\n",
       "      <td>0.271716</td>\n",
       "      <td>0.923866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>-0.372000</td>\n",
       "      <td>0.268219</td>\n",
       "      <td>0.924286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>-0.382700</td>\n",
       "      <td>0.266206</td>\n",
       "      <td>0.925462</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Phase 2 START: reset LR to 0.001 and restart cosine scheduler (6750 steps, 675 warmup)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=13500, training_loss=0.11256539535522461, metrics={'train_runtime': 1032.3108, 'train_samples_per_second': 209.239, 'train_steps_per_second': 13.077, 'total_flos': 1.4208252125184e+16, 'train_loss': 0.11256539535522461, 'epoch': 2.0})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_ag_news.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='238' max='238' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [238/238 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.279926061630249,\n",
       " 'eval_accuracy': 0.935921052631579,\n",
       " 'eval_runtime': 6.0118,\n",
       " 'eval_samples_per_second': 1264.182,\n",
       " 'eval_steps_per_second': 39.589,\n",
       " 'epoch': 2.0}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_ag_news.evaluate(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "train, val, rl, test, model, data_collator, tok = \\\n",
    "    prepare_for_training(\n",
    "        \"sst2\",\n",
    "        \"tiiuae/falcon-rw-1b\", \n",
    "        \"sentence\", \n",
    "        \"label\", \n",
    "        num_virtual_tokens=25,\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18945' max='18945' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18945/18945 26:14, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.682700</td>\n",
       "      <td>0.679614</td>\n",
       "      <td>0.556745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.683300</td>\n",
       "      <td>0.663135</td>\n",
       "      <td>0.564883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.491300</td>\n",
       "      <td>0.395994</td>\n",
       "      <td>0.833911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.407700</td>\n",
       "      <td>0.339540</td>\n",
       "      <td>0.861794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.375700</td>\n",
       "      <td>0.310263</td>\n",
       "      <td>0.874604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.354900</td>\n",
       "      <td>0.297387</td>\n",
       "      <td>0.880784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.337000</td>\n",
       "      <td>0.282504</td>\n",
       "      <td>0.887717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.323000</td>\n",
       "      <td>0.270703</td>\n",
       "      <td>0.889676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.314200</td>\n",
       "      <td>0.265847</td>\n",
       "      <td>0.890882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.312800</td>\n",
       "      <td>0.262738</td>\n",
       "      <td>0.894499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.305700</td>\n",
       "      <td>0.254724</td>\n",
       "      <td>0.898417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>-0.226300</td>\n",
       "      <td>0.251814</td>\n",
       "      <td>0.910075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>-0.223100</td>\n",
       "      <td>0.249124</td>\n",
       "      <td>0.910528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>-0.231700</td>\n",
       "      <td>0.237933</td>\n",
       "      <td>0.914145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>-0.232500</td>\n",
       "      <td>0.235540</td>\n",
       "      <td>0.913240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>-0.249900</td>\n",
       "      <td>0.235957</td>\n",
       "      <td>0.914898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>-0.258600</td>\n",
       "      <td>0.229922</td>\n",
       "      <td>0.917611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>-0.245400</td>\n",
       "      <td>0.210043</td>\n",
       "      <td>0.921214</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Phase 2 START: reset LR to 0.001 and restart cosine scheduler (7578 steps, 757 warmup)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='55' max='55' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [55/55 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.22311674058437347,\n",
       " 'eval_accuracy': 0.9428440366972477,\n",
       " 'eval_runtime': 1.1099,\n",
       " 'eval_samples_per_second': 785.688,\n",
       " 'eval_steps_per_second': 49.556,\n",
       " 'epoch': 5.0}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_sst2 = prepare_trainer(\n",
    "    model, \n",
    "    train, \n",
    "    val, \n",
    "    rl, \n",
    "    data_collator, \n",
    "    tok,\n",
    "    per_device_eval_batch_size=16,\n",
    "    per_device_train_batch_size=16,\n",
    "    num_train_epochs=5,\n",
    "    phase1_epochs=3,\n",
    "    )\n",
    "trainer_sst2.train()\n",
    "trainer_sst2.evaluate(test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
