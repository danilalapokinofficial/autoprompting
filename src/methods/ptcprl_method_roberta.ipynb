{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import Trainer\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch, time, itertools, json\n",
    "from datasets import load_dataset\n",
    "from transformers import (AutoTokenizer,\n",
    "                          AutoModelForSequenceClassification,\n",
    "                          TrainingArguments, Trainer)\n",
    "from peft import PrefixTuningConfig, TaskType, get_peft_model\n",
    "from evaluate import load as load_metric\n",
    "from torch.distributions import Normal\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "metric = load_metric(\"accuracy\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainerCallback, get_scheduler\n",
    "\n",
    "class Phase2LRScheduler(TrainerCallback):\n",
    "    def __init__(self, phase1_epochs: int, phase2_lr: float, warmup_ratio: float):\n",
    "        super().__init__()\n",
    "        self.phase1_epochs = phase1_epochs\n",
    "        self.phase2_lr     = phase2_lr\n",
    "        self.warmup_ratio  = warmup_ratio\n",
    "        self.trainer       = None\n",
    "\n",
    "    def on_epoch_begin(self, args, state, control, **kwargs):\n",
    "        if int(state.epoch) == self.phase1_epochs and self.trainer:\n",
    "            for g in self.trainer.optimizer.param_groups:\n",
    "                g[\"lr\"] = self.phase2_lr\n",
    "\n",
    "            steps_per_epoch = len(self.trainer.get_train_dataloader())\n",
    "            remaining_epochs = args.num_train_epochs - self.phase1_epochs\n",
    "            total_steps_phase2 = remaining_epochs * steps_per_epoch\n",
    "            num_warmup = int(self.warmup_ratio * total_steps_phase2)\n",
    "\n",
    "\n",
    "            self.trainer.lr_scheduler = get_scheduler(\n",
    "                name=\"cosine\",\n",
    "                optimizer=self.trainer.optimizer,\n",
    "                num_warmup_steps=num_warmup,\n",
    "                num_training_steps=total_steps_phase2\n",
    "            )\n",
    "\n",
    "            print(f\"\\n>>> Phase 2 START: reset LR to {self.phase2_lr} and restart cosine scheduler \"\n",
    "                  f\"({total_steps_phase2} steps, {num_warmup} warmup)\\n\")\n",
    "        return control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "from datasets import load_dataset\n",
    "from transformers import DataCollatorWithPadding\n",
    "from transformers import AutoConfig\n",
    "def prepare_for_training(\n",
    "        dataset_name, \n",
    "        model_name,\n",
    "        text_column_name,\n",
    "        label_column_name,\n",
    "        num_virtual_tokens=20, \n",
    "        max_length=128):\n",
    "\n",
    "    if dataset_name == \"sst2\":\n",
    "        raw = load_dataset(\"glue\", dataset_name)\n",
    "    else:\n",
    "        raw = load_dataset(dataset_name, trust_remote_code=True)\n",
    "\n",
    "    tok = AutoTokenizer.from_pretrained(model_name)\n",
    "    if model_name != \"roberta-base\":\n",
    "        tok.pad_token = tok.eos_token\n",
    "    def prep(x):\n",
    "        t = tok(x[text_column_name], padding=\"max_length\", truncation=True, max_length=max_length)\n",
    "        t[\"labels\"]=x[label_column_name]; \n",
    "        return t\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tok, return_tensors=\"pt\")\n",
    "    neseccary_cols = [\"input_ids\",\"attention_mask\",\"labels\"]\n",
    "    ds = raw[\"train\"].shuffle(seed=42).map(\n",
    "        prep, \n",
    "        batched=True, \n",
    "        remove_columns=set(raw[\"train\"].features.keys()).difference(set(neseccary_cols))\n",
    "    )\n",
    "    ds.set_format(\"torch\",columns=neseccary_cols)\n",
    "\n",
    "    split = ds.train_test_split(0.1, shuffle=True)\n",
    "    train_main = split[\"train\"]\n",
    "    val_ds  = split[\"test\"]\n",
    "    split2 = val_ds.train_test_split(test_size=100, shuffle=True)\n",
    "    val_ds = split2[\"train\"]\n",
    "    rl_subset = split2[\"test\"]\n",
    "    test_ds = raw.get('validation') or raw.get(\"test\")\n",
    "    test_ds = test_ds.map(\n",
    "        prep, \n",
    "        batched=True, \n",
    "        remove_columns=set(raw[\"train\"].features.keys()).difference(set(neseccary_cols))\n",
    "    )\n",
    "    test_ds.set_format(\"torch\",columns=neseccary_cols)\n",
    "\n",
    "    num_labels = len(raw[\"train\"].features[label_column_name].names)\n",
    "    \n",
    "    if model_name != \"roberta-base\":\n",
    "        config = AutoConfig.from_pretrained(\n",
    "            \"tiiuae/falcon-rw-1b\",\n",
    "            hidden_size=768,\n",
    "            num_hidden_layers=6,\n",
    "            num_attention_heads=12,\n",
    "            num_key_value_heads=2\n",
    "\n",
    "        )\n",
    "        base = AutoModelForSequenceClassification.from_config(\n",
    "            config,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "        )\n",
    "        tok.pad_token = tok.eos_token\n",
    "        base.config.pad_token_id = tok.pad_token_id\n",
    "    else:\n",
    "        base = AutoModelForSequenceClassification.from_pretrained(\n",
    "            model_name,\n",
    "            num_labels=num_labels,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "        )\n",
    "    if model_name == \"roberta-base\":\n",
    "        for p in base.roberta.parameters(): \n",
    "            p.requires_grad=False\n",
    "    else:\n",
    "        for p in base.parameters(): \n",
    "            p.requires_grad=False\n",
    "\n",
    "    from peft import PrefixTuningConfig, get_peft_model\n",
    "    cfg = PrefixTuningConfig(\n",
    "        task_type=\"SEQ_CLS\",\n",
    "        num_virtual_tokens=num_virtual_tokens,\n",
    "        prefix_projection=False\n",
    "    )\n",
    "    model = get_peft_model(base,cfg)\n",
    "    return train_main, val_ds, rl_subset, test_ds, model, data_collator, tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TwoPhaseTrainer(Trainer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *args,\n",
    "        phase1_epochs: int = 3,\n",
    "        alpha: float = 1.0,\n",
    "        beta: float  = 0.3,\n",
    "        gamma: float = 0.05,\n",
    "        rl_subset_size: int = 32,\n",
    "        rl_dataset=None,\n",
    "        k_negatives: int = 1,\n",
    "        sigma: float = 0.00002,\n",
    "        **kwargs\n",
    "    ):\n",
    "        self.phase1_epochs  = phase1_epochs\n",
    "        self.alpha          = alpha\n",
    "        self.beta           = beta\n",
    "        self.gamma          = gamma\n",
    "        self.rl_subset_size = rl_subset_size\n",
    "        self.rl_dataset     = rl_dataset\n",
    "        self.k_negatives    = k_negatives\n",
    "        self.sigma          = sigma\n",
    "\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "\n",
    "        subset = list(range(min(len(self.rl_dataset), self.rl_subset_size)))\n",
    "        self._rl_loader = DataLoader(\n",
    "            self.rl_dataset.select(subset),\n",
    "            batch_size=self.args.per_device_train_batch_size,\n",
    "            collate_fn=self.data_collator\n",
    "        )\n",
    "        self.prefix_params = []\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if \"prompt_encoder\" in name:\n",
    "                self.prefix_params.append((name, param))\n",
    "\n",
    "\n",
    "    def mutate_prefix_for_contrastive(self):\n",
    "        negs = []\n",
    "        for _ in range(self.k_negatives):\n",
    "            mutated = {}\n",
    "            for name, param in self.prefix_params:\n",
    "                mask = (torch.rand_like(param) > 0.1).float()\n",
    "                mutated[name] = param * mask\n",
    "            negs.append(mutated)\n",
    "        return negs\n",
    "\n",
    "    def compute_contrastive_loss(self, logits_pos, logits_negs, temp=1.0):\n",
    "        pos_norm = F.normalize(logits_pos, dim=-1)\n",
    "        neg_norms = [F.normalize(n, dim=-1) for n in logits_negs]\n",
    "        sim_pos = (pos_norm * pos_norm).sum(-1) / temp\n",
    "        sim_negs = torch.stack([(pos_norm*neg).sum(-1) for neg in neg_norms], dim=1) / temp\n",
    "        loss = -torch.log(sim_pos.exp() / (sim_pos.exp() + sim_negs.exp().sum(1))).mean()\n",
    "        return loss\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def compute_reward(self, model):\n",
    "        was_train = model.training\n",
    "        model.eval()\n",
    "        correct = total = 0\n",
    "        for batch in self._rl_loader:\n",
    "            batch = {k: v.to(model.device) for k,v in batch.items()}\n",
    "            out = model(**batch).logits.argmax(-1)\n",
    "            correct += (out == batch[\"labels\"]).sum().item()\n",
    "            total += out.size(0)\n",
    "        if was_train: model.train()\n",
    "        return correct/total\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        outputs   = model(**inputs)\n",
    "        loss_mle  = outputs.loss\n",
    "\n",
    "        if (self.state.epoch < self.phase1_epochs) or not model.training:\n",
    "            return (loss_mle, outputs) if return_outputs else loss_mle\n",
    "\n",
    "        logits_pos = outputs.logits\n",
    "\n",
    "        original = {n: p.data.clone() for n,p in self.prefix_params}\n",
    "\n",
    "\n",
    "        neg_prefixes = self.mutate_prefix_for_contrastive()\n",
    "        logits_negs = []\n",
    "        for neg in neg_prefixes:\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for name, param in self.prefix_params:\n",
    "                    param.data.copy_(neg[name].data)\n",
    "            logits_negs.append(model(**inputs).logits.detach())\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for name, param in self.prefix_params:\n",
    "                param.data.copy_(original[name])\n",
    "\n",
    "        loss_contrast = self.compute_contrastive_loss(logits_pos, logits_negs)\n",
    "\n",
    "\n",
    "        if self.state.global_step % 100 == 0:\n",
    "            log_probs = []\n",
    "\n",
    "            for name, param in self.prefix_params:\n",
    "                eps   = torch.randn_like(param) * self.sigma\n",
    "                noisy = param + eps\n",
    "                dist  = Normal(loc=param, scale=self.sigma)\n",
    "                log_probs.append(dist.log_prob(noisy).sum())\n",
    "                with torch.no_grad():\n",
    "                    param.data.copy_(noisy.data)\n",
    "\n",
    "            reward = self.compute_reward(model)\n",
    "\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for name, param in self.prefix_params:\n",
    "                    param.data.copy_(original[name])\n",
    "\n",
    "            total_log_prob = torch.stack(log_probs).mean()\n",
    "            loss_rl = - reward * total_log_prob\n",
    "        else:\n",
    "            loss_rl = 0.0\n",
    "\n",
    "        loss = self.alpha*loss_mle + self.beta*loss_contrast + self.gamma*loss_rl\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_trainer(model, \n",
    "                    train_ds, \n",
    "                    val_ds, \n",
    "                    rl_subset, \n",
    "                    data_collator, \n",
    "                    tok, \n",
    "                    alpha=1.0, \n",
    "                    beta=0.1, \n",
    "                    gamma=0.0002,\n",
    "                    rl_subset_size=32,\n",
    "                    k_negatives=7,\n",
    "                    phase1_epochs=3,\n",
    "                    learning_rate=1e-3,\n",
    "                    warmup_ratio=0.1,\n",
    "                    per_device_train_batch_size=16,\n",
    "                    per_device_eval_batch_size=32,\n",
    "                    num_train_epochs=5,\n",
    "                    lr_scheduler_type=\"cosine\",\n",
    "                    sigma=0.1,\n",
    "                    ):\n",
    "\n",
    "    def compute_metrics(eval_pred):\n",
    "        logits, labels = eval_pred\n",
    "        preds = logits.argmax(-1)\n",
    "        return metric.compute(predictions=preds, references=labels)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"two_phase\",\n",
    "        num_train_epochs=num_train_epochs,\n",
    "        per_device_train_batch_size=per_device_train_batch_size,\n",
    "        per_device_eval_batch_size=per_device_eval_batch_size,\n",
    "        learning_rate=learning_rate,\n",
    "        lr_scheduler_type=lr_scheduler_type,\n",
    "        warmup_ratio=warmup_ratio,\n",
    "        eval_strategy=\"epoch\",\n",
    "        metric_for_best_model=\"accuracy\",\n",
    "        fp16=True,\n",
    "        report_to=\"none\",\n",
    "    )\n",
    "    lr_callback = Phase2LRScheduler(\n",
    "        phase1_epochs=phase1_epochs, \n",
    "        phase2_lr=learning_rate, \n",
    "        warmup_ratio=warmup_ratio\n",
    "    )\n",
    "    trainer = TwoPhaseTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_ds,\n",
    "        eval_dataset=val_ds,\n",
    "        rl_dataset=rl_subset,\n",
    "        phase1_epochs=phase1_epochs,\n",
    "        callbacks=[lr_callback],\n",
    "        alpha=alpha, beta=beta, gamma=gamma,\n",
    "        rl_subset_size=rl_subset_size,\n",
    "        k_negatives=k_negatives,\n",
    "        compute_metrics=compute_metrics,\n",
    "        data_collator=data_collator,\n",
    "        sigma=sigma\n",
    "    )\n",
    "\n",
    "    lr_callback.trainer = trainer\n",
    "    return trainer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0625233b6d2e4a6894bee3018899a192",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5452 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "train_ds, val_ds, rl_subset, test_ds, model, data_collator, tok \\\n",
    "    = prepare_for_training(\"trec\", \"roberta-base\", \"text\", \"coarse_label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "trainer_trec = prepare_trainer(model, train_ds, val_ds, rl_subset, data_collator, tok, phase1_epochs=15, num_train_epochs=20, per_device_train_batch_size=2, per_device_eval_batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='49060' max='49060' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [49060/49060 23:25, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.616600</td>\n",
       "      <td>0.585644</td>\n",
       "      <td>0.810000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.585600</td>\n",
       "      <td>0.558537</td>\n",
       "      <td>0.860000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.586300</td>\n",
       "      <td>0.552811</td>\n",
       "      <td>0.840000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.541800</td>\n",
       "      <td>0.551066</td>\n",
       "      <td>0.860000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.533600</td>\n",
       "      <td>0.532745</td>\n",
       "      <td>0.850000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.540300</td>\n",
       "      <td>0.517641</td>\n",
       "      <td>0.860000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.522700</td>\n",
       "      <td>0.482401</td>\n",
       "      <td>0.860000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.491400</td>\n",
       "      <td>0.489579</td>\n",
       "      <td>0.860000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.473100</td>\n",
       "      <td>0.473438</td>\n",
       "      <td>0.880000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.473800</td>\n",
       "      <td>0.384764</td>\n",
       "      <td>0.890000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.440000</td>\n",
       "      <td>0.385699</td>\n",
       "      <td>0.880000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.451200</td>\n",
       "      <td>0.396479</td>\n",
       "      <td>0.890000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.446100</td>\n",
       "      <td>0.445834</td>\n",
       "      <td>0.880000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.385600</td>\n",
       "      <td>0.464894</td>\n",
       "      <td>0.870000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.390500</td>\n",
       "      <td>0.414776</td>\n",
       "      <td>0.880000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.338600</td>\n",
       "      <td>0.365535</td>\n",
       "      <td>0.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>0.360400</td>\n",
       "      <td>0.402965</td>\n",
       "      <td>0.880000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.366600</td>\n",
       "      <td>0.453005</td>\n",
       "      <td>0.880000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>0.383200</td>\n",
       "      <td>0.347025</td>\n",
       "      <td>0.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.344400</td>\n",
       "      <td>0.382174</td>\n",
       "      <td>0.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>0.346700</td>\n",
       "      <td>0.308736</td>\n",
       "      <td>0.910000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.330500</td>\n",
       "      <td>0.329240</td>\n",
       "      <td>0.910000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>0.347800</td>\n",
       "      <td>0.294804</td>\n",
       "      <td>0.930000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.345500</td>\n",
       "      <td>0.362659</td>\n",
       "      <td>0.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>0.311800</td>\n",
       "      <td>0.375342</td>\n",
       "      <td>0.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.338600</td>\n",
       "      <td>0.360808</td>\n",
       "      <td>0.920000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>0.294600</td>\n",
       "      <td>0.313028</td>\n",
       "      <td>0.920000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>0.292000</td>\n",
       "      <td>0.389793</td>\n",
       "      <td>0.910000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29000</td>\n",
       "      <td>0.291000</td>\n",
       "      <td>0.342506</td>\n",
       "      <td>0.920000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.285800</td>\n",
       "      <td>0.313713</td>\n",
       "      <td>0.930000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31000</td>\n",
       "      <td>0.293600</td>\n",
       "      <td>0.288617</td>\n",
       "      <td>0.940000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>0.272000</td>\n",
       "      <td>0.321963</td>\n",
       "      <td>0.930000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33000</td>\n",
       "      <td>0.251600</td>\n",
       "      <td>0.301058</td>\n",
       "      <td>0.930000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>0.270300</td>\n",
       "      <td>0.311480</td>\n",
       "      <td>0.930000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35000</td>\n",
       "      <td>0.271800</td>\n",
       "      <td>0.336721</td>\n",
       "      <td>0.920000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36000</td>\n",
       "      <td>0.262100</td>\n",
       "      <td>0.339602</td>\n",
       "      <td>0.920000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37000</td>\n",
       "      <td>0.053600</td>\n",
       "      <td>0.340531</td>\n",
       "      <td>0.920000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38000</td>\n",
       "      <td>-0.302300</td>\n",
       "      <td>0.246003</td>\n",
       "      <td>0.950000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39000</td>\n",
       "      <td>-0.255900</td>\n",
       "      <td>0.300819</td>\n",
       "      <td>0.930000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>-0.299800</td>\n",
       "      <td>0.327674</td>\n",
       "      <td>0.920000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41000</td>\n",
       "      <td>-0.284500</td>\n",
       "      <td>0.283322</td>\n",
       "      <td>0.930000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42000</td>\n",
       "      <td>-0.291300</td>\n",
       "      <td>0.340075</td>\n",
       "      <td>0.920000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43000</td>\n",
       "      <td>-0.291600</td>\n",
       "      <td>0.317235</td>\n",
       "      <td>0.930000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44000</td>\n",
       "      <td>-0.324500</td>\n",
       "      <td>0.277253</td>\n",
       "      <td>0.940000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45000</td>\n",
       "      <td>-0.318700</td>\n",
       "      <td>0.263762</td>\n",
       "      <td>0.930000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46000</td>\n",
       "      <td>-0.321700</td>\n",
       "      <td>0.237336</td>\n",
       "      <td>0.950000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47000</td>\n",
       "      <td>-0.332500</td>\n",
       "      <td>0.288890</td>\n",
       "      <td>0.930000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48000</td>\n",
       "      <td>-0.371700</td>\n",
       "      <td>0.274395</td>\n",
       "      <td>0.940000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49000</td>\n",
       "      <td>-0.328000</td>\n",
       "      <td>0.272473</td>\n",
       "      <td>0.940000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Phase 2 START: reset LR to 0.001 and restart cosine scheduler (12265 steps, 1226 warmup)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=49060, training_loss=0.2110375697108322, metrics={'train_runtime': 1405.248, 'train_samples_per_second': 69.824, 'train_steps_per_second': 34.912, 'total_flos': 6454345983528960.0, 'train_loss': 0.2110375697108322, 'epoch': 20.0})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_trec.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.2112264335155487,\n",
       " 'eval_accuracy': 0.96,\n",
       " 'eval_runtime': 3.3671,\n",
       " 'eval_samples_per_second': 148.498,\n",
       " 'eval_steps_per_second': 74.249,\n",
       " 'epoch': 20.0}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_trec.evaluate(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "train_ds, val_ds, rl_subset, test_ds, model, data_collator, tok \\\n",
    "    = prepare_for_training(\"ag_news\", \"roberta-base\", \"text\", \"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "trainer_ag_news = prepare_trainer(model, train_ds, val_ds, rl_subset, data_collator, tok, k_negatives=4, num_train_epochs=2, phase1_epochs=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13500' max='13500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13500/13500 14:42, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.385100</td>\n",
       "      <td>1.385020</td>\n",
       "      <td>0.330000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.859400</td>\n",
       "      <td>0.487562</td>\n",
       "      <td>0.840000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.401600</td>\n",
       "      <td>0.402590</td>\n",
       "      <td>0.870000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.373800</td>\n",
       "      <td>0.356760</td>\n",
       "      <td>0.890000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.356700</td>\n",
       "      <td>0.325648</td>\n",
       "      <td>0.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.350100</td>\n",
       "      <td>0.321817</td>\n",
       "      <td>0.890000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.193400</td>\n",
       "      <td>0.306405</td>\n",
       "      <td>0.910000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>-0.054800</td>\n",
       "      <td>0.319089</td>\n",
       "      <td>0.910000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>-0.076700</td>\n",
       "      <td>0.289830</td>\n",
       "      <td>0.910000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>-0.082000</td>\n",
       "      <td>0.283822</td>\n",
       "      <td>0.920000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>-0.076300</td>\n",
       "      <td>0.295194</td>\n",
       "      <td>0.920000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>-0.091900</td>\n",
       "      <td>0.295614</td>\n",
       "      <td>0.910000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>-0.099300</td>\n",
       "      <td>0.288925</td>\n",
       "      <td>0.920000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Phase 2 START: reset LR to 0.001 and restart cosine scheduler (6750 steps, 675 warmup)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=13500, training_loss=0.2780038450735587, metrics={'train_runtime': 882.3207, 'train_samples_per_second': 244.809, 'train_steps_per_second': 15.301, 'total_flos': 1.4208252125184e+16, 'train_loss': 0.2780038450735587, 'epoch': 2.0})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_ag_news.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='238' max='238' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [238/238 00:06]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.285306841135025,\n",
       " 'eval_accuracy': 0.9203947368421053,\n",
       " 'eval_runtime': 6.2499,\n",
       " 'eval_samples_per_second': 1216.02,\n",
       " 'eval_steps_per_second': 38.081,\n",
       " 'epoch': 2.0}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_ag_news.evaluate(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "train, val, rl, test, model, data_collator, tok = \\\n",
    "    prepare_for_training(\n",
    "        \"sst2\",\n",
    "        \"roberta-base\", \n",
    "        \"sentence\", \n",
    "        \"label\", \n",
    "        num_virtual_tokens=20,\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18945' max='18945' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18945/18945 15:27, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.686700</td>\n",
       "      <td>0.680515</td>\n",
       "      <td>0.554333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.678300</td>\n",
       "      <td>0.661271</td>\n",
       "      <td>0.577543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.529700</td>\n",
       "      <td>0.411103</td>\n",
       "      <td>0.830445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.422200</td>\n",
       "      <td>0.360108</td>\n",
       "      <td>0.853504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.400700</td>\n",
       "      <td>0.337047</td>\n",
       "      <td>0.864356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.367400</td>\n",
       "      <td>0.317015</td>\n",
       "      <td>0.875659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.355800</td>\n",
       "      <td>0.311153</td>\n",
       "      <td>0.882442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.335900</td>\n",
       "      <td>0.293904</td>\n",
       "      <td>0.882743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.338900</td>\n",
       "      <td>0.285507</td>\n",
       "      <td>0.885757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.323600</td>\n",
       "      <td>0.283495</td>\n",
       "      <td>0.889375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.318900</td>\n",
       "      <td>0.281142</td>\n",
       "      <td>0.888169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>-0.208200</td>\n",
       "      <td>0.271681</td>\n",
       "      <td>0.899800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>-0.218600</td>\n",
       "      <td>0.270347</td>\n",
       "      <td>0.906006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>-0.221700</td>\n",
       "      <td>0.260075</td>\n",
       "      <td>0.906760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>-0.231200</td>\n",
       "      <td>0.257444</td>\n",
       "      <td>0.909884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>-0.233100</td>\n",
       "      <td>0.257244</td>\n",
       "      <td>0.915774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>-0.234000</td>\n",
       "      <td>0.249656</td>\n",
       "      <td>0.916788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>-0.240400</td>\n",
       "      <td>0.249956</td>\n",
       "      <td>0.916336</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Phase 2 START: reset LR to 0.001 and restart cosine scheduler (7578 steps, 757 warmup)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='55' max='55' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [55/55 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.24063801765441895,\n",
       " 'eval_accuracy': 0.9403669724770642,\n",
       " 'eval_runtime': 0.7886,\n",
       " 'eval_samples_per_second': 1105.718,\n",
       " 'eval_steps_per_second': 69.741,\n",
       " 'epoch': 5.0}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_sst2 = prepare_trainer(\n",
    "    model, \n",
    "    train, \n",
    "    val, \n",
    "    rl, \n",
    "    data_collator, \n",
    "    tok,\n",
    "    per_device_eval_batch_size=16,\n",
    "    per_device_train_batch_size=16,\n",
    "    num_train_epochs=5,\n",
    "    phase1_epochs=3,\n",
    "    )\n",
    "trainer_sst2.train()\n",
    "trainer_sst2.evaluate(test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
